# -*- coding: utf-8 -*-
"""Final_Costco_Stock_Modeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZR4dCqgLlX_W9ju52rj7hGUeSZLYuQr3
"""

# Core Libraries
import numpy as np
import pandas as pd
from datetime import datetime

# Plotting & Visualization
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib as mpl

# Financial Data
import yfinance as yf

# Statistical Tools
import scipy.stats as ss
import statsmodels.api as sm

# Scikit-learn Models & Tools
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.cross_decomposition import PLSRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.base import clone

# XGBoost
import xgboost as xgb

# === Configure plotting ===
plt.rcParams['figure.figsize'] = (16, 6)

# === Load Costco FeatureMart CSV ===
DATA = pd.read_csv('Costco_FeatureMart.csv')  # use the remembered uploaded file
DATA['Date'] = pd.to_datetime(DATA['Date'])                  # ensure proper datetime format
X = DATA.drop(columns=['Date']).bfill()                      # feature matrix

# === Download COST stock data ===
stock_symbol = 'COST'
start_date = datetime(2020, 1, 1)
end_date = datetime(2024, 12, 31)

stock = yf.download(stock_symbol, start=start_date, end=end_date)
close_prices = stock['Close'].ffill()                        # handle missing values

# === Compute log returns (already a Series) ===
log_returns = np.log(close_prices).diff().fillna(0)          # y values for modeling
y = log_returns                                              # directly use as Series

# === Align X and y by date ===
# Convert feature dates to index
DATA['Date'] = pd.to_datetime(DATA['Date'])
X.index = DATA['Date']                                       # assign date index to X
X_aligned, y_aligned = X.align(y, join='inner', axis=0)      # inner join to keep only matched dates

# === Final shapes for sanity check ===
print("X shape:", X_aligned.shape)
print("y shape:", y_aligned.shape)
print("Aligned date range:", X_aligned.index.min(), "to", X_aligned.index.max())

"""### Benchmark Model: Ordinary Linear Regression"""

# === Step 1: Add constant to aligned features ===
X_const = sm.add_constant(X_aligned)

# === Step 2: Initial regression on all features (for t-value thresholding)
benchmark_prep = sm.OLS(y_aligned, X_const).fit()

# === Step 3: Feature selection (|t| >= 1.96)
benchmark_select = X_const.columns[np.abs(benchmark_prep.tvalues) >= 1.96]
x_selected = X_const[benchmark_select]

# === Step 4: Final model on selected features
benchmark = sm.OLS(y_aligned, x_selected).fit()
print(benchmark.summary())  # <-- only keeping this one

# === Step 5: Predictions and correlation
y_hat_benchmark = benchmark.predict(x_selected)
corr_benchmark, _ = ss.pearsonr(np.ravel(y_hat_benchmark), np.ravel(y_aligned))

# === Step 6: Results
print(f'\nBenchmark: corr(Y, Y_pred) = {corr_benchmark:.4f}')
print(f'Hard Thresholding selected {len(benchmark_select)} features:\n{benchmark_select.values}')

"""### Ridge Regression"""

# Step 1: Run Ridge regression to select features
a = 0.5  # Regularization strength
model2_prep = linear_model.Ridge(alpha=a, fit_intercept=False).fit(X_aligned, y_aligned)

# Step 2: Select features with coefficients significantly away from 0
model2_select = X_aligned.columns[np.abs(model2_prep.coef_) >= 0.001]
x_selected = X_aligned[model2_select]

# Step 3: Run OLS on selected features
model2 = sm.OLS(y_aligned, x_selected).fit()

# Step 4: Predictions and correlation
y_pred_model2 = model2.predict(x_selected)
corr_model2, _ = ss.pearsonr(np.ravel(y_pred_model2), np.ravel(y_aligned))

# Step 5: Output results
print(model2.summary())
print(f'\nModel 2 Ridge Regression: corr(Y, Y_pred) = {corr_model2:.4f}')
print(f'Ridge Regression selected {len(model2_select)} features:\n{model2_select.values}')

"""### LASSO"""

# Step 1: Run LASSO regression to shrink coefficients
a = 0.5
model3_prep = linear_model.Lasso(alpha=a, fit_intercept=False).fit(X_aligned, y_aligned)

# Step 2: Select features with non-zero coefficients
model3_select = X_aligned.columns[np.abs(model3_prep.coef_) != 0.0]
x_selected = X_aligned[model3_select]

# Step 3: Run OLS on selected features
model3 = sm.OLS(y_aligned, x_selected).fit()

# Step 4: Predictions and correlation
y_pred_model3 = model3.predict(x_selected)
corr_model3, _ = ss.pearsonr(np.ravel(y_pred_model3), np.ravel(y_aligned))

# Step 5: Output results
print(model3.summary())
print(f'\nModel 3 LASSO: corr(Y, Y_pred) = {corr_model3:.4f}')
print(f'LASSO selected {len(model3_select)} features:\n{model3_select.values}')

"""### Elastic Net"""

# Step 1: Fit ElasticNet on aligned data
a = 0.5
model4_prep = linear_model.ElasticNet(alpha=a, fit_intercept=False).fit(X_aligned, y_aligned)

# Step 2: Select features with non-zero coefficients
model4_select = X_aligned.columns[np.abs(model4_prep.coef_) != 0.0]
x_selected = X_aligned[model4_select]

# Step 3: Refit with OLS
model4 = sm.OLS(y_aligned, x_selected).fit()

# Step 4: Evaluation
y_pred_model4 = model4.predict(x_selected)
corr_model4, _ = ss.pearsonr(np.ravel(y_pred_model4), np.ravel(y_aligned))

# Step 5: Output
print(model4.summary())
print(f'\nModel 4 ElasticNet: corr(Y, Y_pred) = {corr_model4:.4f}')
print(f'ElasticNet selected {len(model4_select)} features:\n{model4_select.values}')

"""### Least Angle Regression"""

# Step 1: Fit Least Angle Regression on aligned data
model1_prep = linear_model.Lars().fit(X_aligned, y_aligned)

# Step 2: Select features with non-zero coefficients
model1_select = X_aligned.columns[np.abs(model1_prep.coef_) >= 0.001]
x_selected = X_aligned[model1_select]

# Step 3: Refit selected features with OLS
model1 = sm.OLS(y_aligned, x_selected).fit()

# Step 4: Predict and evaluate
y_pred_model1 = model1.predict(x_selected)
corr_model1, _ = ss.pearsonr(np.ravel(y_pred_model1), np.ravel(y_aligned))

# Step 5: Output results
print(model1.summary())
print(f'\nModel 1 LARS: corr(Y, Y_pred) = {corr_model1:.4f}')
print(f'LARS selected {len(model1_select)} features:\n{model1_select.values}\n')

"""# Random Forest: Feature Importance"""

# Figure configuration


plt.rcParams['figure.figsize'] = (16, 6)

# define dataset
start_date = datetime(2020, 1, 1)
end_date = datetime(2024, 12, 31)
stock_symbol = 'COST'
stock = yf.download(stock_symbol, start=start_date, end=end_date)
STOCK_raw = stock.copy()
STOCK = STOCK_raw.copy()

# Load Costco feature mart
DATA_raw = pd.read_csv('Costco_FeatureMart.csv', index_col=0)
DATA = DATA_raw.copy()

# NORMALIZATION
for column in DATA.columns:
    DATA[column] = (DATA[column] - DATA[column].mean()) / DATA[column].std()

# NORMALIZATION for stock
for column in STOCK.columns:
    STOCK[column] = (STOCK[column] - STOCK[column].mean()) / STOCK[column].std()

# ‚úÖ Align features and target on common date index
X = DATA.drop(columns=['Date'], errors='ignore').bfill()
X.index = pd.to_datetime(X.index)
y = STOCK['Close']
y.index = pd.to_datetime(y.index)
X, y = X.align(y, join='inner', axis=0)

# define the model
model = RandomForestRegressor(
    bootstrap=True,
    criterion='squared_error',
    max_depth=None,
    max_features=5,
    max_leaf_nodes=None,
    min_impurity_decrease=0.0,
    min_samples_leaf=1,
    min_samples_split=2,
    min_weight_fraction_leaf=0.0,
    n_estimators=100,
    n_jobs=None,
    oob_score=False,
    random_state=None,
    verbose=0,
    warm_start=False
)

# fit the model
model.fit(X, y)

# 1. average feature importance
df_feature_importance = pd.DataFrame(model.feature_importances_, index=X.columns, \
                                     columns=['feature importance']).sort_values('feature importance', ascending=False)
print(df_feature_importance)

# 2. all feature importance for each tree
df_feature_all = pd.DataFrame([tree.feature_importances_ for tree in model.estimators_], columns=X.columns)
df_feature_all.head()
# Melted data i.e., long format
df_feature_long = pd.melt(df_feature_all,var_name='feature name', value_name='values')
print(df_feature_long.iloc[0:104])

# 3. visualize feature importance (run each line sequentially)
# (1) bar chart
df_feature_importance.plot(kind='bar')

# (2) box plot
sns.boxplot(x="feature name", y="values", data=df_feature_long, order=df_feature_importance.index)

# (3) strip plot
sns.stripplot(x="feature name", y="values", data=df_feature_long, order=df_feature_importance.index)

# (5) all above in one plot
fig, axes = plt.subplots(3, 1, figsize=(16, 8))
df_feature_importance.plot(kind='bar', ax=axes[0], title='Plots Comparison for Feature Importance')
sns.boxplot(ax=axes[1], x="feature name", y="values", data=df_feature_long, order=df_feature_importance.index)
sns.stripplot(ax=axes[2], x="feature name", y="values", data=df_feature_long, order=df_feature_importance.index)
plt.tight_layout()

# Configure plot size
plt.rcParams['figure.figsize'] = (16, 6)

# --------------------------------------------------
# STEP 1: Download COSTCO stock data
# --------------------------------------------------
start_date = datetime(2020, 1, 1)
end_date = datetime(2024, 12, 31)

stock = yf.download('Cost', start=start_date, end=end_date)
stock = stock[['Close']].reset_index()            # Make 'Date' a column
stock.columns = ['Date', 'Close']                 # Rename for consistency
stock['Date'] = pd.to_datetime(stock['Date'])     # Ensure datetime type

# --------------------------------------------------
# STEP 2: Load Costco FeatureMart and preprocess
# --------------------------------------------------
data_raw = pd.read_csv('Costco_FeatureMart.csv')
data_raw['Date'] = pd.to_datetime(data_raw['Date'])  # Convert to datetime

# --------------------------------------------------
# STEP 3: Merge on 'Date' to align features with target
# --------------------------------------------------
merged = pd.merge(data_raw, stock, how='inner', on='Date')

# --------------------------------------------------
# STEP 4: Normalize features (Z-score), excluding 'Date' & 'Close'
# --------------------------------------------------
features = merged.drop(columns=['Date', 'Close']).copy()

for col in features.columns:
    features[col] = (features[col] - features[col].mean()) / features[col].std()

# --------------------------------------------------
# STEP 5: Prepare X and y
# --------------------------------------------------
X = features.bfill()                 # Fill any missing values
y = merged['Close'].values           # Target: COSTCO closing prices

# --------------------------------------------------
# STEP 6: Define and train XGBoost model
# --------------------------------------------------
model = xgb.XGBRegressor(
    objective='reg:squarederror',
    n_estimators=20,
    learning_rate=0.05,
    max_depth=3,
    alpha=10,
    eta=0.1,
    subsample=0.7,
    colsample_bytree=0.8,
    reg_lambda=1,
    gamma=0
)

model.fit(X, y)

# --------------------------------------------------
# STEP 7: Plot feature importance
# --------------------------------------------------
xgb.plot_importance(model)
plt.title("XGBoost Feature Importance")
plt.tight_layout()
plt.show()

# 1. average feature importance
df_feature_importance = pd.DataFrame(model.feature_importances_, index=X.columns, \
                                     columns=['feature importance']).sort_values('feature importance', ascending=False)
print(df_feature_importance)

# 2. visualize feature importance (run each line sequentially)
# (1) bar chart
df_feature_importance.plot(kind='bar')

# exhaustively search for the optimal hyperparameters
from sklearn.model_selection import GridSearchCV
# set up our search grid
param_grid = {"max_depth":    [3, 5, 10],
              "n_estimators": [10, 50, 100],
              "learning_rate": [0.15, 0.3]}

# try out every combination of the above values
regressor = xgb.XGBRegressor(eval_metric='rmsle')
search = GridSearchCV(regressor, param_grid, cv=5).fit(X, y)

print("The best hyperparameters are ",search.best_params_)
regressor = xgb.XGBRegressor(learning_rate = search.best_params_["learning_rate"],
                           n_estimators  = search.best_params_["n_estimators"],
                           max_depth     = search.best_params_["max_depth"],)

regressor.fit(X, y)


from xgboost import plot_importance
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
plt.rcParams.update({'font.size': 12})

fig, ax = plt.subplots(figsize=(12,6))
plot_importance(regressor, max_num_features=8, ax=ax)
plt.show()

# Load feature mart
data = pd.read_csv("Costco_FeatureMart.csv", index_col=0)

# Convert index to datetime
data.index = pd.to_datetime(data.index)

# Fill any missing values
data = data.bfill()

# Normalize each feature (z-score)
for col in data.columns:
    data[col] = (data[col] - data[col].mean()) / data[col].std()

selected_features_rf = [
    'SP500', 'DEXJPUS', 'DAAA', 'CBBTCUSD', 'RF'
]

# Download closing price and compute log return
costco = yf.download("COST", start=data.index.min(), end=data.index.max())
log_return = np.log(costco["Close"]).diff().fillna(0)
log_return.name = "LogReturn"

# Intersect with X
common_idx = data.index.intersection(log_return.index)
X = data[selected_features_rf].loc[common_idx]
y = log_return.loc[common_idx]

X_train = X[X.index < "2023-01-01"]
X_test = X[X.index >= "2023-01-01"]

y_train = y[X.index < "2023-01-01"]
y_test = y[X.index >= "2023-01-01"]

models = {
    "Linear Regression": LinearRegression(),
    "Ridge Regression": Ridge(alpha=1.0),
    "Lasso Regression": Lasso(alpha=0.01),
    "Random Forest": RandomForestRegressor(n_estimators=100, max_features=5, random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42),
    "PLS Regression": PLSRegression(n_components=5)
}

results = []

for name, model in models.items():
    model.fit(X_train, y_train)

    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)

    rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))
    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))


    results.append({
        "Model": name,
        "RMSE Train": round(rmse_train, 4),
        "RMSE Test": round(rmse_test, 4)
    })

results_df = pd.DataFrame(results)
print(results_df)

# STEP 1: Load and prepare feature data
data = pd.read_csv("Costco_FeatureMart.csv", index_col=0)
data.index = pd.to_datetime(data.index)
data = data.bfill()

# Z-score normalization
data = (data - data.mean()) / data.std()

# STEP 2: Download actual Costco closing price and compute log return
costco_price = yf.download("COST", start=data.index.min(), end=data.index.max())["Close"]
log_return = np.log(costco_price).diff().fillna(0)
log_return.name = "LogReturn"

# STEP 3: Align features and target
rolling_features = ['SP500', 'DEXJPUS', 'DAAA', 'CBBTCUSD', 'RF']
X_rolling = data[rolling_features].copy()
common_idx = X_rolling.index.intersection(log_return.index)
X_rolling = X_rolling.loc[common_idx]
y_rolling = log_return.loc[common_idx]

# Parameters
window_size = 252  # 1-year rolling window
rolling_days = 50  # Evaluate last 50 days

# Define models
models = {
    "Linear Regression": LinearRegression(),
    "Ridge Regression": Ridge(alpha=1.0),
    "Lasso Regression": Lasso(alpha=0.01),
    "PLS Regression": PLSRegression(n_components=2)
}

# Rolling RMSE results
rolling_rmse_results = []

for name, model in models.items():
    preds, actuals, dates = [], [], []

    for i in range(len(X_rolling) - window_size - rolling_days):
        train_X = X_rolling.iloc[i : i + window_size]
        train_y = y_rolling.iloc[i : i + window_size]
        test_X = X_rolling.iloc[i + window_size : i + window_size + 1]
        test_y = y_rolling.iloc[i + window_size : i + window_size + 1]

        if len(test_X) == 0:
            break

        m = clone(model)
        m.fit(train_X, train_y)

        preds.append(m.predict(test_X)[0])
        actuals.append(test_y.values[0])
        dates.append(test_y.index[0])

    rmse = np.sqrt(mean_squared_error(actuals, preds))
    rolling_rmse_results.append({"Model": name, "Rolling RMSE": round(rmse, 4)})

# Display results
results_df = pd.DataFrame(rolling_rmse_results)
print("üîÅ Rolling RMSE (Last 50 Days):\n")
print(results_df.to_string(index=False))

def plot_strategy_vs_bnh(model, model_name, X_data, y_data, window_size=252, rolling_days=50):
    from sklearn.base import clone
    import numpy as np
    import matplotlib.pyplot as plt

    preds, actuals, dates = [], [], []

    for i in range(len(X_data) - window_size - rolling_days):
        train_X = X_data.iloc[i : i + window_size]
        train_y = y_data.iloc[i : i + window_size]
        test_X = X_data.iloc[i + window_size : i + window_size + 1]
        test_y = y_data.iloc[i + window_size : i + window_size + 1]

        if len(test_X) == 0:
            break

        m = clone(model)
        m.fit(train_X, train_y)

        preds.append(m.predict(test_X)[0])
        actuals.append(test_y.values[0])
        dates.append(test_y.index[0])

    # Generate signals
    signal = np.sign(np.diff(preds, prepend=preds[0]))
    strategy_return = signal * np.diff(actuals, prepend=actuals[0])
    bnh_return = np.diff(actuals, prepend=actuals[0])

    # Cumulative return
    cumulative_strategy = np.cumsum(strategy_return)
    cumulative_bnh = np.cumsum(bnh_return)

    # Plot
    plt.figure(figsize=(12, 6))
    plt.plot(dates, cumulative_strategy, label="üìà Strategy")
    plt.plot(dates, cumulative_bnh, label="üíº Buy & Hold")
    plt.title(f"üìä Cumulative Return: Strategy vs Buy-and-Hold\n({model_name})")
    plt.xlabel("Date")
    plt.ylabel("Cumulative Price Change")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()

from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.cross_decomposition import PLSRegression

# Models to test
models = {
    "Linear Regression": LinearRegression(),
    "Ridge Regression": Ridge(alpha=1.0),
    "Lasso Regression": Lasso(alpha=0.01),-
    "PLS Regression": PLSRegression(n_components=2)
}

# Data inputs
X_selected = data[['SP500', 'DEXJPUS', 'DAAA', 'CBBTCUSD', 'RF']]
y_target = data['Costco']

# Run for each model
for name, model in models.items():
    plot_strategy_vs_bnh(model, name, X_selected, y_target)

# STEP 1: Load and prepare data
data = pd.read_csv("Costco_FeatureMart.csv", index_col=0)
data.index = pd.to_datetime(data.index)
data = data.bfill()

# Z-score normalization
for col in data.columns:
    data[col] = (data[col] - data[col].mean()) / data[col].std()

# Create Next-Day Return target
data['Next_Return'] = data['Costco'].pct_change().shift(-1)
data.dropna(inplace=True)

# Features and target
features = ['SP500', 'DEXJPUS', 'DAAA', 'CBBTCUSD', 'RF']
X = data[features]
y = data['Next_Return']

# Train-test split
X_train = X[X.index < "2023-01-01"]
X_test = X[X.index >= "2023-01-01"]
y_train = y[X.index < "2023-01-01"]
y_test = y[X.index >= "2023-01-01"]

# STEP 2: Define and train models
models = {
    "Linear Regression": LinearRegression(),
    "Ridge Regression": Ridge(alpha=1.0),
    "Lasso Regression": Lasso(alpha=0.01),
    "PLS Regression": PLSRegression(n_components=2)
}

for name, model in models.items():
    model.fit(X_train, y_train)

# STEP 3: Simulate trading and plot PnL
plt.figure(figsize=(12, 6))

for name, model in models.items():
    y_pred = model.predict(X_test)

    # Buy/sell signal
    signal = np.sign(y_pred)

    true_return = y_test.values
    strategy_return = signal * true_return

    # Cumulative PnL
    cum_strategy = np.cumsum(strategy_return)
    cum_bnh = np.cumsum(true_return)

    plt.plot(y_test.index, cum_strategy, label=f"{name} Strategy")
    print(f"{name} Final Strategy PnL: {cum_strategy[-1]:.4f}")
    print(f"{name} Final Buy & Hold PnL: {cum_bnh[-1]:.4f}")
    print("-" * 40)

# Plot Buy & Hold once
plt.plot(y_test.index, np.cumsum(y_test.values), label="Buy & Hold", linestyle='--')

plt.title("üìä Strategy vs Buy-and-Hold (Next-Day Return Prediction)")
plt.xlabel("Date")
plt.ylabel("Cumulative Return")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

# STEP 1: Load and prepare data
data = pd.read_csv("Costco_FeatureMart.csv", index_col=0)
data.index = pd.to_datetime(data.index)
data = data.bfill()

# Z-score normalization
for col in data.columns:
    data[col] = (data[col] - data[col].mean()) / data[col].std()

# Create Next-Day Return target
data['Next_Return'] = data['Costco'].pct_change().shift(-1)
data.dropna(inplace=True)

# Features and target
features = ['SP500', 'DEXJPUS', 'DAAA', 'CBBTCUSD', 'RF']
X = data[features]
y = data['Next_Return']

# Train-test split
X_train = X[X.index < "2023-01-01"]
X_test = X[X.index >= "2023-01-01"]
y_train = y[X.index < "2023-01-01"]
y_test = y[X.index >= "2023-01-01"]

# STEP 2: Define and train models
models = {
    "Linear Regression": LinearRegression(),
    "Ridge Regression": Ridge(alpha=1.0),
    "Lasso Regression": Lasso(alpha=0.01),
    "PLS Regression": PLSRegression(n_components=2)
}

for name, model in models.items():
    model.fit(X_train, y_train)

# STEP 3: Fetch actual stock prices
cost = yf.download("COST", start="2023-01-01")['Close']
brkb = yf.download("BRK-B", start="2023-01-01")['Close']

# Align with y_test index
cost = cost.loc[y_test.index]
brkb = brkb.loc[y_test.index]

# Normalize to start at 1, scale to match cumulative return space
normalized_cost = cost / cost.iloc[0] * np.cumsum(y_test.values)[-1]
normalized_brkb = brkb / brkb.iloc[0] * np.cumsum(y_test.values)[-1]

# STEP 4: Plot strategies and actual stock lines
plt.figure(figsize=(14, 6))

for name, model in models.items():
    y_pred = model.predict(X_test)
    signal = np.sign(y_pred)
    strategy_return = signal * y_test.values
    cum_strategy = np.cumsum(strategy_return)
    cum_bnh = np.cumsum(y_test.values)

    plt.plot(y_test.index, cum_strategy, label=f"{name} Strategy")
    print(f"{name} Final Strategy PnL: {cum_strategy[-1]:.4f}")
    print(f"{name} Final Buy & Hold PnL: {cum_bnh[-1]:.4f}")
    print("-" * 40)

# Add Buy & Hold
plt.plot(y_test.index, cum_bnh, label="Buy & Hold", linestyle="--", linewidth=2)

# Add Costco Stock Line
plt.plot(normalized_cost.index, normalized_cost, label="Costco Stock", linestyle="dotted", linewidth=2)

# Add Warren Buffett (BRK-B) Stock Line
plt.plot(normalized_brkb.index, normalized_brkb, label="Berkshire Hathaway (BRK-B)", linestyle="dashdot", linewidth=2)

# Final plot styling
plt.title("Strategies vs Buy-and-Hold vs Costco vs BRK-B")
plt.xlabel("Date")
plt.ylabel("Cumulative Return / Normalized Price")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()